# -*- coding: utf-8 -*-
"""TFM_TimeSeries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14JAcpdxzqg-Kh8HL8bXHYoNJ2iVhV-c_
"""

import pandas as pd
#Acceso a Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Lista de años para los que cargar los datos
years = ['2014', '2015']

# Cargar y combinar los archivos de Excel con las columnas especificadas
data_pv_list = [pd.read_excel(f'/content/drive/My Drive/MASTER BIG DATA/Sunlab-Faro-PV-{year}.xlsx',
                              usecols=['Datetime', 'A_Optimal - Power DC [W]'])
               for year in years]

# Concatenar los datos en un solo DataFrame
data_pv = pd.concat(data_pv_list, ignore_index=True)

df = data_pv

print('Primeras filas', df.head())

df.shape

# Convertir la columna 'Datetime' a tipo datetime si no está en ese formato
#df['Datetime'] = pd.to_datetime(df['Datetime'])

# Establecer 'Datetime' como el índice
#df.set_index('Datetime', inplace=True)

# Agrupar los datos por día y sumar los valores
data = df.resample('W').sum()  # Cambia 'sum' por cualquier otra función de agregación que necesites
#data = df
data.plot()

data.describe()

# Imputación de valores nulos con interpolación lineal
data_interpolado = data.interpolate(method='linear')
# Calcular el rango intercuartílico (IQR)
Q1 = data_interpolado.quantile(0.25)
Q3 = data_interpolado.quantile(0.75)
IQR = Q3 - Q1

# Filtrar outliers
data_sin_outliers = data_interpolado[~((data_interpolado < (Q1 - 1.5 * IQR)) | (data_interpolado > (Q3 + 1.5 * IQR))).any(axis=1)]

df_clean = data_sin_outliers

df_clean.describe()

na_values = df_clean.isnull().sum()
print('Número de nulos: ', na_values)

data = df_clean

data.shape

data.plot()

"""## ARIMA- SARIMAX... ACF, PCF..."""

from sklearn.model_selection import train_test_split

# Dividir los datos en entrenamiento (80%) y prueba (20%)
train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False, random_state=42)

import statsmodels.api as sm
import matplotlib.pyplot as plt

# Realizar la descomposición estacional
decomposition = sm.tsa.seasonal_decompose(data, model='additive', period=12)  # Ajusta el período según sea necesario

# Graficar los resultados
decomposition.plot()
plt.show()

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Gráfico de Autocorrelación (ACF)
plot_acf(data)
plt.show()

# Gráfico de Parcial Autocorrelación (PACF)
plot_pacf(data)
plt.show()

from statsmodels.tsa.stattools import adfuller

# Realizar la prueba de Dickey-Fuller aumentada
result = adfuller(data)
print('Estadística de prueba de Dickey-Fuller:', result[0])
print('Valor p:', result[1])
print('Valores críticos:', result[4])

# Un valor p inferior a 0.05 sugiere que la serie es estacionaria.

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.model_selection import train_test_split

# Dividir los datos en conjuntos de entrenamiento y prueba (80% para entrenamiento, 20% para prueba)
train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)

# Define la variable de salida (y) para el modelo
training_y = train_data['A_Optimal - Power DC [W]']

# Define el modelo ARIMA con los parámetros p, d, q
modelo_arima = ARIMA(training_y, order=(1,0,1))

# Ajusta el modelo
resultado_arima = modelo_arima.fit()

# Muestra un resumen del modelo
print(resultado_arima.summary())


# Realizar predicciones en los datos de prueba
predictions = resultado_arima.forecast(steps=len(test_data))

# Graficar datos de entrenamiento, prueba y predicciones
plt.figure(figsize=(12, 6))
plt.plot(train_data.index, training_y, label='Datos de entrenamiento', color='blue')
plt.plot(test_data.index, test_data['A_Optimal - Power DC [W]'], label='Datos de prueba', color='orange')
plt.plot(test_data.index, predictions, label='Predicciones', color='green', linestyle='--')

# Configurar el gráfico
plt.title('Datos de entrenamiento, prueba y predicciones')
plt.xlabel('Fecha')
plt.ylabel('Valor')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Dividir los datos en conjuntos de entrenamiento y prueba (80% para entrenamiento, 20% para prueba)
train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)

# Define la variable de salida (y) para el modelo
training_y = train_data['A_Optimal - Power DC [W]']

# Define el modelo ARIMA con los parámetros p, d, q
modelo_arima = ARIMA(training_y, order=(1,0,1))

# Ajusta el modelo
resultado_arima = modelo_arima.fit()

# Realizar predicciones en los datos de prueba
forecast = resultado_arima.get_forecast(steps=len(test_data))
predictions = forecast.predicted_mean
conf_int = forecast.conf_int()

# Graficar datos de entrenamiento, prueba y predicciones
plt.figure(figsize=(12, 6))
plt.plot(train_data.index, training_y, label='Datos de entrenamiento', color='navy')
plt.plot(test_data.index, test_data['A_Optimal - Power DC [W]'], label='Datos de prueba', color='orange')
plt.plot(test_data.index, predictions, label='Predicciones', color='green', linestyle='--')

# Graficar intervalo de confianza
plt.fill_between(test_data.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='green', alpha=0.3)

# Configurar el gráfico
plt.title('Datos de entrenamiento, prueba y predicciones')
plt.xlabel('Fecha')
plt.ylabel('Valor')
plt.legend()
plt.show()

# Calcular el error cuadrático medio (MSE)
arima_mse = mean_squared_error(test_data['A_Optimal - Power DC [W]'], predictions)
print("Error cuadrático medio (MSE) de ARIMA:", arima_mse)

# Calcular el coeficiente de determinación (R^2)
arima_r2 = r2_score(test_data['A_Optimal - Power DC [W]'], predictions)
print("Coeficiente de determinación (R^2) de ARIMA:", arima_r2)

"""### Normalización de datos"""

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Seleccionar las columnas que deseas normalizar
columnas_a_normalizar = ['A_Optimal - Power DC [W]']  # Ajusta según tus datos

# Crear un objeto MinMaxScaler
scaler = MinMaxScaler()

# Ajustar el scaler a los datos y transformarlos
data[columnas_a_normalizar] = scaler.fit_transform(data[columnas_a_normalizar])

# Ahora los datos en 'columnas_a_normalizar' están normalizados en el rango [0, 1]

data.head()

# Gráfico de Autocorrelación (ACF)
plot_acf(data, lags = 30)
plt.show()

# Gráfico de Parcial Autocorrelación (PACF)
plot_pacf(data, lags = 30)
plt.show()

from statsmodels.tsa.arima.model import ARIMA

# Define el modelo ARIMA con los parámetros p, d, q
modelo_arima = ARIMA(data, order=(2, 1, 5))

# Ajusta el modelo
resultado_arima = modelo_arima.fit()

# Muestra un resumen del modelo
print(resultado_arima.summary())

# Función para graficar los residuos del modelo ARIMA
def plot_residuals(result):
    # Extrae los residuos del modelo
    residuos = result.resid

    # Grafica los residuos
    plt.figure(figsize=(12, 6))
    plt.plot(residuos, label='Residuos')
    plt.axhline(0, color='r', linestyle='--', linewidth=1)
    plt.title('Residuos del modelo ARIMA')
    plt.xlabel('Índice')
    plt.ylabel('Residuos')
    plt.legend()
    plt.show()

    # Realiza un histograma de los residuos
    plt.figure(figsize=(12, 6))
    plt.hist(residuos, bins=30, edgecolor='black')
    plt.title('Histograma de los residuos')
    plt.xlabel('Residuos')
    plt.ylabel('Frecuencia')
    plt.show()


# Llamada a la función para plotear los residuos
plot_residuals(resultado_arima)

"""## Pdarima"""

!pip install pmdarima

import pandas as pd
import pmdarima as pm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Divide los datos en entrenamiento (70%) y prueba (30%)
train_data = data[:int(0.9 * len(data))]
test_data = data[int(0.9 * len(data)):]

# Realizar la búsqueda automática de parámetros para ARIMA
model = pm.auto_arima(
    train_data,
    start_p=0,
    start_q=0,
    max_p=15,  # Aumenta los límites de búsqueda
    max_q=15,
    d=None,
    seasonal=True,
    m=52,  # Periodo estacional, ajusta según tu serie
    stepwise=True,  # Stepwise para búsqueda eficiente
    trace=True,
    error_action='ignore',  # Manejar errores en la búsqueda
    information_criterion='aic',  # Criterio de información
    method='nm'  # Método de optimización, ajusta si es necesario
)

# Imprimir el resumen del modelo óptimo
print(model.summary())

# Realizar predicciones en los datos de prueba
predictions, pred_int = model.predict(n_periods=len(test_data), return_conf_int=True)

# Verificar la longitud de las predicciones y los datos de prueba
assert len(predictions) == len(test_data), "La longitud de las predicciones no coincide con los datos de prueba."

# Calcular el error cuadrático medio (MSE) entre las predicciones y los datos de prueba
mse = mean_squared_error(test_data, predictions)
print(f'Error Cuadrático Medio (MSE): {mse}')

# Calcula el R2 score
r2 = r2_score(test_data, predictions)
print(f'R2 Score: {r2}')

# Graficar datos de entrenamiento, prueba y predicciones
plt.figure(figsize=(10, 6))
plt.plot(train_data, label='Datos de entrenamiento', color='navy')
plt.plot(test_data, label='Datos de prueba', color='orange')
plt.plot(test_data.index, predictions, label='Predicciones', color='green', linestyle='--', linewidth=1.5)

# Graficar intervalo de predicción de confianza
plt.fill_between(test_data.index, pred_int[:, 0], pred_int[:, 1], color='green', alpha=0.25)

# Configurar el gráfico
plt.title('Predicción de potencia total semanal entre los años 2014 y 2015')
plt.xlabel('Fecha')
plt.ylabel('Potencia (W)')
plt.legend(loc='upper left')
plt.show()